
---
title: "Maximum Likelihood Estimation and Bayesian Statistics"
author: "Rebecca C. Steorts"
output: 
     beamer_presentation:
      includes: 
          in_header: custom2.tex
font-size: 8px
---



Agenda
===
- Maximum Likelihood Esimation
- Unbiased Estimators


Traditional inference
===
You are given **data** $X$ and there is an **unknown parameter** you wish to estimate **$\theta$**

How would you estimate $\theta$?

- Find an unbiased estimator of $\theta$. 
- Find the maximum likelihood estimate (MLE) of $\theta$ by looking at the likelihood of the data. 
- Suppose that $\hat{\theta}$ estimates $\theta.$ Note: $\hat{\theta}$ may depend on the data $x_{1:n} = x_1, \ldots x_n.$

Unbiased Estimator
===
Recall that $\hat{\theta}$ is an unbiased estimator of $\theta$ if 

\begin{equation}
E[\hat{\theta}] = \theta.
\end{equation}.

Maximum  Likelihood Estimation
===

For each sample point $x_{1:n},$ let $\hat{\theta}$ be a parameter value at which $p(x_{1:n} \mid \theta)$ attains it's maximum as a function of $\theta,$ with $x_{1:n}$ held fixed. A \emph{maximum likelihood esimator (MLE)} of the parameter $\theta$ based on a sample $x_{1:n}$ is $\hat{\theta}.$

Find the MLE
===

The solution to the MLE are the possible candidates ($\theta$) that solve 

\begin{equation}
\frac{\partial p(x_{1:n} \mid \theta)}{\partial \theta} = 0.
\end{equation}

Solution to the above equation are only possible candidates for the MLE since the first derivative being 0 is a necessary condition for a maximum (but not a sufficient one). 

Our job is to find a global maximum. Thus, we need to ensure that we haven't found a local one.  

MLE of Normal distribution
===

Consider $$X_1, \ldots, X_n \stackrel{iid}{\sim} \text{Normal}(\theta, 1).$$

Show that the MLE is $\hat{\theta} =\bar{x}.$

MLE of Normal distribution
===

\begin{equation}
p(x_{1:n} \mid \theta) = (2 \pi)^{-n/2} \times \exp \{\frac{-1}{2} \sum_i (x_i - \theta )^2  \}
\end{equation}

Consider 

\begin{equation}
\log p(x_{1:n}) = -n/2 \log (2\pi) - \frac{1}{2} \sum_i (x_i - \theta )^2 
\end{equation}

MLE of Normal distribution
===

\begin{equation}
\frac{\partial p(x_{1:n} \mid \theta)}{\partial \theta} = \sum_i(x_i - \theta)
\end{equation}

This implies that 

$$\sum_i(x_i - \theta) = 0 \implies \hat{\theta} = \bar{x}.$$

MLE of Normal distribution
===

Consider 

$$\frac{\partial^2 p(x_{1:n} \mid \theta)}{\partial \theta^2} = -n < 0.$$
Thus, our solution is unique. 

Exercise
===

Show that 

$$\hat{\theta} = \bar{x}$$ is an unbiased estimator for $\theta.$

Proof. 

$$E[\hat{\theta}] = E[\bar{x}] = \frac{1}{n}\sum_iE[x_i] = \frac{1}{n} \sum_i \theta = \theta.$$

Thus, we have showed that the MLE is an unbiased estimator for $\theta.$





Normal-Normal model
===

Suppose that  $$X_1, \ldots, X_n \stackrel{iid}{\sim} \text{Normal}(\theta, 1),$$
where we now consider $$\theta \stackrel{ind}{\sim} \text{Normal}(\mu, \tau^2).$$


Let $\lambda = 1$ and $\lambda_o = 1/\tau^2.$

Recall that from module 3, $$\theta \mid x_{1:n} \sim N(M,L^{-1}),$$ where
$$L = n\lambda + \lambda_o$$
and $$M = \frac{n \lambda \bar{x} + \lambda_o\mu}{n\lambda + \lambda_o}.$$


Normal MLE
===

Observe that 

$$
M = \frac{n \lambda \bar{x} + \lambda_o\mu}{n\lambda + \lambda_o}
= \frac{n \lambda \hat{\theta} + \lambda_o\mu}{n\lambda + \lambda_o}
= \frac{n\lambda}{n\lambda + \lambda_o} \hat{\theta} + \frac{\lambda_o}{n\lambda + \lambda_o}\mu.
$$

Thus, we can write the posterior mean as a function of the MLE and the prior mean $\mu.$

Bernoulli-Bayes Estimation
===

$$X_1, \ldots, X_n \stackrel{iid}{\sim} \text{Bernoulli}(\theta).$$
This implies that $Y = \sum_i X_i \sim Binomial(n, \theta).$

It can be shown that the MLE for $\theta$ is $\bar{x} = y/n.$

Consider $\theta \sim \text{Beta}(a,b).$

Recall that $$\theta \mid y \sim Beta(y + a, n - y + b).$$

Bernoulli-Bayes Estimation
===

Consider the posterior mean

$$
\begin{aligned}
E[\theta \mid y] &= \frac{y + a}{y + a +  n - y + b}  = \frac{y + a}{a +  n  + b} \\
&= \frac{y}{a + b + n} + \frac{a}{a + b + n} \\
&= 
\frac{y}{n} \times \frac{n}{a + b + n} + \frac{a}{a+b} \times \frac{a+b}{a + b + n} \\
&= 
MLE \times \frac{n}{a + b + n} + priorMean \times \frac{a+b}{a + b + n}
\end{aligned}
$$
Thus, we have written the posterior mean as a linear comboination of the MLE and prior mean with weights being determined by a,b, and n.

Evaluation of Estimators
===

How do we evaluate estimators? We often use the mean sauared error. 

$$\text{MSE}(\hat{\theta}) = E_\theta [(\hat{\theta} - \theta)^2].$$

Observe that 

$$\text{MSE}(\hat{\theta}) = Var_\theta(\hat{\theta}) + E_\theta[(\hat{\theta} - \theta)^2] = Var_\theta(\hat{\theta}) + \text{Bias}_\theta (\hat{\theta}),$$

where the $$\text{Bias}_\theta (\hat{\theta}) = E_\theta(\hat{\theta}) - \theta.$$

For a more in depth treatment of MSE and bias, see Section 7.3.1, Casella and Berger, p. 330 - 334.
Binomial MLE
===
Let $$X_1, \ldots, X_n \stackrel{iid}{\sim} \text{Bernoulli}(\theta).$$

Show that the MLE is $\hat{\theta} =\bar{x}.$

Proof: Casella and Berger, Example 7.2.7, page 317-318.

Normal-Normal model
===
Suppose that  $$X_1, \ldots, X_n \stackrel{iid}{\sim} \text{Normal}(\theta, \sigma^2),$$
where $\theta, \sigma^2$ are both unknown. 

Show that $(\bar{x}, n^{-1} \sum_i (x_i - \bar{x})^2))$ are the MLE's for $(\theta, \sigma^2).$

Proof: Casella and Berger, Example 7.2.7, page 317-318.

Invariance property of MLE's
===

If $\hat{\theta}$ is the MLE of $\theta$, then for any function $g(\theta)$, the MLE of $g(\theta)$ 
is the MLE of $g(\hat{\theta}).$

Proof: Theorem 7.2.10, Casella and Berger, page 318.










